{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd9595ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Saved scaler: ./scaler_ASTERUSDT_1min.gz\n",
      "shapes â†’ X_train (6603, 48, 5) y_train (6603,) w_train (6603,) | X_val (692, 48, 5) y_val (692,) w_val (692,) | X_test (1801, 48, 5) y_test (1801,) w_test (1801,)\n",
      "[001/50] train_loss 0.6462 | val_loss 0.5588 | val_acc 0.717 | val_bal_acc 0.702 | val_auc 0.768 | best_metric 0.7024 | no_improve 0/8\n",
      "[005/50] train_loss 0.5442 | val_loss 0.5199 | val_acc 0.718 | val_bal_acc 0.714 | val_auc 0.797 | best_metric 0.7209 | no_improve 1/8\n",
      "[010/50] train_loss 0.5337 | val_loss 0.5082 | val_acc 0.754 | val_bal_acc 0.732 | val_auc 0.802 | best_metric 0.7319 | no_improve 0/8\n",
      "[015/50] train_loss 0.5320 | val_loss 0.5135 | val_acc 0.730 | val_bal_acc 0.727 | val_auc 0.801 | best_metric 0.7319 | no_improve 5/8\n",
      "â¹ï¸ Early stopped at epoch 18 (best at 10, best_metric=0.7319).\n",
      "âœ… Restored best validation model (epoch 10, best_metric=0.7319).\n",
      "ğŸ’¾ Model saved to ./Models\\LSTM_classify_ASTERUSDT_2509292209.pt\n",
      "\n",
      "[Test] loss 0.6401 | acc 0.670 | bal_acc 0.680 | auc 0.775\n",
      "\n",
      "Classification report (labels: 0=Short, 1=Long):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Short(0)      0.760     0.597     0.669      1004\n",
      "     Long(1)      0.600     0.763     0.672       797\n",
      "\n",
      "    accuracy                          0.670      1801\n",
      "   macro avg      0.680     0.680     0.670      1801\n",
      "weighted avg      0.689     0.670     0.670      1801\n",
      "\n",
      "Confusion matrix:\n",
      " [[599 405]\n",
      " [189 608]]\n"
     ]
    }
   ],
   "source": [
    "#0.Import Libraries & Basic Settings\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"torch\")\n",
    "\n",
    "import os, datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# æ—¢å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n",
    "from Generate_Features_and_Labels import generate_labels_and_features, add_trade_weights\n",
    "from LSTM_model_classify import LSTMClassifier, make_latest_lstm_window\n",
    "\n",
    "#1.Set Parameters\n",
    "# ãƒ‡ãƒ¼ã‚¿\n",
    "symbol = 'ASTERUSDT'\n",
    "csv_filename = 'Market_Bybit_ASTERUSDT_1min_20250901-20250926'\n",
    "csv_path = f'01.data/{csv_filename}.csv'\n",
    "model_min = 1\n",
    "\n",
    "# ãƒ©ãƒ™ãƒ«ç”Ÿæˆ\n",
    "twap_forward_period = 60\n",
    "twap_lookback_period = 60\n",
    "\n",
    "# æå¤±ã‚¦ã‚§ã‚¤ãƒˆè¨­å®š\n",
    "weight_fee = 0.0011\n",
    "weight_slip = 0.000\n",
    "weight_quantile = 0.95\n",
    "weight_gamma = 0.9995\n",
    "weight_alpha = 0.9\n",
    "weight_min = 0.0\n",
    "weitght_max = 3.0\n",
    "weight_sigma_scale = 0.1\n",
    "\n",
    "# LSTM\n",
    "LSTM_lookback   = 48\n",
    "LSTM_hidden     = 128\n",
    "LSTM_layers     = 2\n",
    "LSTM_dropout    = 0.2\n",
    "LSTM_num_classes= 2\n",
    "LSTM_learnRate  = 1e-3\n",
    "LSTM_gamma      = 0.985\n",
    "LSTM_epochs     = 50\n",
    "LSTM_train_ratio= 0.8\n",
    "LSTM_val_frac   = 0.1\n",
    "LSTM_patience   = 8\n",
    "LSTM_min_delta  = 1e-3\n",
    "LSTM_batch = 256\n",
    "LSTM_seed = 18\n",
    "\n",
    "# ç‰¹å¾´/ãƒ©ãƒ™ãƒ«\n",
    "Features = ['O_log', 'H_log', 'L_log', 'C_log', 'V_log']\n",
    "Label    = 'label'\n",
    "\n",
    "# ä¿å­˜\n",
    "models_dir  = \"./Models\"\n",
    "scaler_path = f'./scaler_{symbol}_{model_min}min.gz'   # â˜…è¶³ã«åˆã‚ã›ã¦å‘½å\n",
    "batch_size  = LSTM_batch\n",
    "seed        = LSTM_seed\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === Seedå›ºå®š ===\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "#3.Utility Functions (only within ipynb)\n",
    "def f3(x):\n",
    "    try:\n",
    "        if x is None: return \"nan\"\n",
    "        if isinstance(x, float) and (np.isnan(x) or np.isinf(x)): return \"nan\"\n",
    "        return f\"{x:.3f}\"\n",
    "    except Exception:\n",
    "        return \"nan\"\n",
    "\n",
    "def make_sequences(X2d: np.ndarray, y1d: np.ndarray, w1d: np.ndarray, seq_len: int):\n",
    "    if len(X2d) < seq_len:\n",
    "        raise ValueError(f\"ãƒ‡ãƒ¼ã‚¿ä¸è¶³: len={len(X2d)}, SEQ_LEN={seq_len}\")\n",
    "    Xs, ys, ws = [], [], []\n",
    "    for i in range(seq_len-1, len(X2d)):\n",
    "        Xs.append(X2d[i-seq_len+1:i+1])\n",
    "        ys.append(y1d[i])\n",
    "        ws.append(w1d[i])   # ãƒ©ãƒ™ãƒ«ã¨åŒã˜æ™‚ç‚¹ã®é‡ã¿ã‚’ä»˜ä¸\n",
    "    return np.stack(Xs), np.array(ys), np.array(ws)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, device, criterion=None, weighted=False):\n",
    "    \"\"\"\n",
    "    loader: (X,y) ã¾ãŸã¯ (X,y,w) ã®ã„ãšã‚Œã‹\n",
    "    criterion:\n",
    "      - None ãªã‚‰æå¤±ã¯è¨ˆç®—ã—ãªã„\n",
    "      - reduction='none' ã®CEã‚’æ¸¡ã™ã¨ weighted=True ã§é‡ã¿ä»˜ãæå¤±ã‚’è¨ˆç®—\n",
    "      - ãã‚Œä»¥å¤–ãªã‚‰é€šå¸¸å¹³å‡æå¤±\n",
    "    weighted: True ã®ã¨ã (X,y,w) ãªã‚‰é‡ã¿ä»˜ãå¹³å‡æå¤±ã§é›†è¨ˆ\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_y, all_pred, all_prob = [], [], []\n",
    "    total_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            if len(batch) == 3:\n",
    "                Xb, yb, wb = batch\n",
    "                wb = wb.to(device)\n",
    "            else:\n",
    "                Xb, yb = batch\n",
    "                wb = None\n",
    "\n",
    "            Xb = Xb.to(device); yb = yb.to(device)\n",
    "            logits = model(Xb)\n",
    "\n",
    "            # --- æå¤± ---\n",
    "            if criterion is not None:\n",
    "                if weighted and wb is not None and getattr(criterion, 'reduction', 'mean') == 'none':\n",
    "                    loss_vec = criterion(logits, yb)                 # (B,)\n",
    "                    denom = torch.clamp(wb.sum(), min=1e-8)\n",
    "                    loss = (loss_vec * wb).sum() / denom\n",
    "                else:\n",
    "                    # é€šå¸¸ï¼ˆé‡ã¿ç„¡è¦–ï¼‰ã®å¹³å‡æå¤±\n",
    "                    loss = criterion(logits, yb)\n",
    "                total_loss += float(loss.detach().cpu())\n",
    "                n_batches += 1\n",
    "\n",
    "            # --- äºˆæ¸¬/ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç”¨ ---\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            pred  = probs.argmax(dim=1)\n",
    "\n",
    "            all_y.append(yb.cpu()); all_pred.append(pred.cpu()); all_prob.append(probs.cpu())\n",
    "\n",
    "    all_y    = torch.cat(all_y).numpy()\n",
    "    all_pred = torch.cat(all_pred).numpy()\n",
    "    all_prob = torch.cat(all_prob).numpy()\n",
    "\n",
    "    acc = (all_y == all_pred).mean()\n",
    "    bal_acc, auc = float('nan'), float('nan')\n",
    "    if len(np.unique(all_y)) >= 2:\n",
    "        try:\n",
    "            bal_acc = balanced_accuracy_score(all_y, all_pred)\n",
    "        except: pass\n",
    "        try:\n",
    "            if all_prob.shape[1] == 2:\n",
    "                auc = roc_auc_score(all_y, all_prob[:, 1])\n",
    "            else:\n",
    "                auc = roc_auc_score(all_y, all_prob, multi_class=\"ovr\")\n",
    "        except: pass\n",
    "\n",
    "    avg_loss = total_loss / max(1, n_batches) if criterion is not None else float('nan')\n",
    "    return avg_loss, acc, bal_acc, auc, all_y, all_pred, all_prob\n",
    "\n",
    "\n",
    "#4.Read csv and generate features/labels\n",
    "df_raw = pd.read_csv(csv_path)\n",
    "df_raw['timestamp'] = pd.to_datetime(df_raw['timestamp'])\n",
    "\n",
    "df_feat = generate_labels_and_features(df_raw, twap_forward_period, twap_lookback_period)\n",
    "df_w = add_trade_weights(df_feat, fee=weight_fee, slip=weight_slip, rem_horizon=twap_forward_period, quantile=weight_quantile,\n",
    "                         gamma=weight_gamma, alpha=weight_alpha, wmin=weight_min, wmax=weitght_max, sigma_scale=weight_sigma_scale)\n",
    "\n",
    "use_cols = ['timestamp'] + Features + [Label, 'weight', 'weight_base', 'weight_rem']\n",
    "data = df_w[use_cols].dropna().reset_index(drop=True)\n",
    "data.head()\n",
    "\n",
    "#5.Split / Standardize & Sequence\n",
    "split_idx = int(len(data) * LSTM_train_ratio)\n",
    "train_all = data.iloc[:split_idx].copy()\n",
    "test_df   = data.iloc[split_idx:].copy()\n",
    "val_idx   = int(len(train_all) * (1 - LSTM_val_frac))\n",
    "train_df  = train_all.iloc[:val_idx].copy()\n",
    "val_df    = train_all.iloc[val_idx:].copy()\n",
    "\n",
    "# Standardizeï¼ˆç‰¹å¾´ã ã‘ï¼‰\n",
    "scaler = StandardScaler()\n",
    "X_train_2d = scaler.fit_transform(train_df[Features].values)\n",
    "X_val_2d   = scaler.transform(val_df[Features].values)\n",
    "X_test_2d  = scaler.transform(test_df[Features].values)\n",
    "\n",
    "y_train_1d = train_df[Label].astype(int).values\n",
    "y_val_1d   = val_df[Label].astype(int).values\n",
    "y_test_1d  = test_df[Label].astype(int).values\n",
    "\n",
    "w_train_1d = train_df['weight'].astype(float).values\n",
    "w_val_1d   = val_df['weight'].astype(float).values\n",
    "w_test_1d  = test_df['weight'].astype(float).values\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(\"Saved scaler:\", scaler_path)\n",
    "\n",
    "# Sequencesï¼ˆé‡ã¿è¾¼ã¿ï¼‰\n",
    "X_train, y_train, w_train = make_sequences(X_train_2d, y_train_1d, w_train_1d, LSTM_lookback)\n",
    "X_val,   y_val,   w_val   = make_sequences(X_val_2d,   y_val_1d,   w_val_1d,   LSTM_lookback)\n",
    "X_test,  y_test,  w_test  = make_sequences(X_test_2d,  y_test_1d,  w_test_1d,  LSTM_lookback)\n",
    "\n",
    "print(\"shapes â†’\",\n",
    "      \"X_train\", X_train.shape, \"y_train\", y_train.shape, \"w_train\", w_train.shape,\n",
    "      \"| X_val\", X_val.shape,   \"y_val\",   y_val.shape,   \"w_val\",   w_val.shape,\n",
    "      \"| X_test\", X_test.shape, \"y_test\",  y_test.shape,  \"w_test\",  w_test.shape)\n",
    "\n",
    "\n",
    "#6.Tensor/Dataloader\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "w_train_t = torch.tensor(w_train, dtype=torch.float32)\n",
    "\n",
    "X_val_t = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_t = torch.tensor(y_val, dtype=torch.long)\n",
    "w_val_t = torch.tensor(w_val, dtype=torch.float32)\n",
    "\n",
    "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_t = torch.tensor(y_test, dtype=torch.long)\n",
    "w_test_t = torch.tensor(w_test, dtype=torch.float32)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_t, y_train_t, w_train_t), batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(TensorDataset(X_val_t,   y_val_t,   w_val_t),   batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(TensorDataset(X_test_t,  y_test_t,  w_test_t),  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "#7.Model\n",
    "model = LSTMClassifier(\n",
    "    input_dim=len(Features),\n",
    "    hidden_dim=LSTM_hidden,\n",
    "    num_layers=LSTM_layers,\n",
    "    num_classes=LSTM_num_classes,\n",
    "    dropout=LSTM_dropout\n",
    ").to(device)\n",
    "\n",
    "criterion_ce = nn.CrossEntropyLoss(reduction='none')\n",
    "optimizer = optim.Adam(model.parameters(), lr=LSTM_learnRate)\n",
    "scheduler = ExponentialLR(optimizer, gamma=LSTM_gamma)\n",
    "\n",
    "#8.Training\n",
    "patience  = LSTM_patience\n",
    "min_delta = LSTM_min_delta\n",
    "no_improve = 0\n",
    "best_val = None\n",
    "best_state = None\n",
    "best_epoch = None\n",
    "\n",
    "def select_metric(val_balacc, val_acc):\n",
    "    m = val_balacc\n",
    "    if isinstance(m, float) and (np.isnan(m) or np.isinf(m)):\n",
    "        m = val_acc\n",
    "    return float(m)\n",
    "\n",
    "use_amp = (device.type == \"cuda\")\n",
    "scaler_amp = torch.amp.GradScaler(\"cuda\") if use_amp else None\n",
    "\n",
    "for epoch in range(1, LSTM_epochs + 1):\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        Xb, yb, wb = batch\n",
    "        Xb = Xb.to(device); yb = yb.to(device); wb = wb.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if use_amp:\n",
    "            with torch.amp.autocast(\"cuda\"):\n",
    "                logits = model(Xb)                             # (B, 2)\n",
    "                loss_vec = criterion_ce(logits, yb)           # (B,)\n",
    "                # æ­£è¦åŒ–ï¼šé‡ã¿ã®å’Œã§å‰²ã‚‹ï¼ˆã‚¼ãƒ­åˆ†æ¯å›é¿ï¼‰\n",
    "                denom = torch.clamp(wb.sum(), min=1e-8)\n",
    "                loss = (loss_vec * wb).sum() / denom\n",
    "            scaler_amp.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler_amp.step(optimizer)\n",
    "            scaler_amp.update()\n",
    "        else:\n",
    "            logits = model(Xb)\n",
    "            loss_vec = criterion_ce(logits, yb)\n",
    "            denom = torch.clamp(wb.sum(), min=1e-8)\n",
    "            loss = (loss_vec * wb).sum() / denom\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        total += float(loss.detach().cpu())\n",
    "\n",
    "    train_loss = total / max(1, len(train_loader))\n",
    "    scheduler.step()\n",
    "\n",
    "    criterion_eval = nn.CrossEntropyLoss(reduction='none')\n",
    "    val_loss, val_acc, val_balacc, val_auc, *_ = evaluate(model, val_loader, device, criterion_eval, weighted=True)\n",
    "\n",
    "    metric = select_metric(val_balacc, val_acc)\n",
    "\n",
    "    improved = (best_val is None) or (metric > best_val + min_delta)\n",
    "\n",
    "\n",
    "    if improved:\n",
    "        best_val = metric\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        best_epoch = epoch\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "\n",
    "    if epoch % 5 == 0 or epoch == 1:\n",
    "        print(f\"[{epoch:03d}/{LSTM_epochs}] \"\n",
    "              f\"train_loss {train_loss:.4f} | \"\n",
    "              f\"val_loss {val_loss:.4f} | \"\n",
    "              f\"val_acc {f3(val_acc)} | \"\n",
    "              f\"val_bal_acc {f3(val_balacc)} | \"\n",
    "              f\"val_auc {f3(val_auc)} | \"\n",
    "              f\"best_metric {best_val:.4f} | \"\n",
    "              f\"no_improve {no_improve}/{patience}\")\n",
    "\n",
    "    if no_improve >= patience:\n",
    "        print(f\"â¹ï¸ Early stopped at epoch {epoch} (best at {best_epoch}, best_metric={best_val:.4f}).\")\n",
    "        break\n",
    "\n",
    "\n",
    "# === Best restoreï¼ˆåŒã˜ï¼‰ ===\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "    print(f\"âœ… Restored best validation model (epoch {best_epoch}, best_metric={best_val:.4f}).\")\n",
    "\n",
    "# === Save path: æœªå®šç¾©å¤‰æ•°ã‚’ä½¿ã‚ãªã„å‘½åã«ä¿®æ­£ ===\n",
    "ts = datetime.datetime.now().strftime(\"%y%m%d%H%M\")\n",
    "# ä¾‹1: ã”å¸Œæœ›ã®æœ€å°å½¢å¼\n",
    "model_path = os.path.join(models_dir, f\"LSTM_classify_{symbol}_{ts}.pt\")\n",
    "# ä¾‹2: ã‚‚ã†å°‘ã—æƒ…å ±ã‚’æŒãŸã›ã‚‹ãªã‚‰ï¼ˆä»»æ„ï¼‰\n",
    "# model_path = os.path.join(models_dir, f\"LSTM_classify_{symbol}_{model_min}m_lb{LSTM_lookback}_{ts}.pt\")\n",
    "\n",
    "torch.save({\n",
    "    \"epoch\": best_epoch,\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"best_metric\": best_val,\n",
    "    \"params\": {\n",
    "        \"lookback\": LSTM_lookback,\n",
    "        \"hidden\": LSTM_hidden,\n",
    "        \"layers\": LSTM_layers,\n",
    "        \"dropout\": LSTM_dropout,\n",
    "        \"num_classes\": LSTM_num_classes,\n",
    "        \"input_dim\": len(Features),\n",
    "        \"features\": Features,           # â˜…è¿½åŠ ï¼ˆä»»æ„ï¼‰\n",
    "        \"scaler_path\": scaler_path,     # â˜…è¿½åŠ ï¼ˆä»»æ„ï¼‰\n",
    "        \"train_ratio\": LSTM_train_ratio,# â˜…è¿½åŠ ï¼ˆä»»æ„ï¼‰\n",
    "    }\n",
    "}, model_path)\n",
    "print(f\"ğŸ’¾ Model saved to {model_path}\")\n",
    "\n",
    "#9.Model Evaluation\n",
    "criterion_eval = nn.CrossEntropyLoss()  # â˜…è©•ä¾¡ç”¨ã«è¿½åŠ \n",
    "\n",
    "criterion_eval = nn.CrossEntropyLoss(reduction='none')\n",
    "test_loss, test_acc, test_balacc, test_auc, y_true, y_pred, y_prob = evaluate(model, test_loader, device, criterion_eval, weighted=True)\n",
    "\n",
    "print(f\"\\n[Test] loss {test_loss:.4f} | acc {f3(test_acc)} | bal_acc {f3(test_balacc)} | auc {f3(test_auc)}\")\n",
    "\n",
    "print(\"\\nClassification report (labels: 0=Short, 1=Long):\")\n",
    "print(classification_report(y_true, y_pred, digits=3, target_names=[\"Short(0)\", \"Long(1)\"]))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e65d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10. Inference / Build df_predicted\n",
    "import torch\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_inference_build_df(\n",
    "    df_raw: pd.DataFrame,\n",
    "    model_path: str,\n",
    "    scaler_path: str,\n",
    "    features: list,\n",
    "    label_col: str,\n",
    "    seq_len: int,\n",
    "    device: Optional[torch.device] = None,\n",
    "    batch_size: int = 1024,\n",
    "    keep_prob: bool = False,   # Trueãªã‚‰ç¢ºä¿¡åº¦ã‚‚å‡ºåŠ›\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    å­¦ç¿’æ¸ˆã¿LSTMåˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€df_rawå…¨ä½“ã«å¯¾ã—ã¦ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°æ¨è«–ã€‚\n",
    "    è¿”ã‚Šå€¤ã¯ [timestamp, O,H,L,C,V, Pred, Label] (+ ProbLong ä»»æ„) ã‚’æŒã¤ df_predictedã€‚\n",
    "    \"\"\"\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # --- 1) ç‰¹å¾´ãƒ»ãƒ©ãƒ™ãƒ«å†ç”Ÿæˆï¼ˆå­¦ç¿’æ™‚ã¨åŒã˜é–¢æ•°ã‚’åˆ©ç”¨ï¼‰ ---\n",
    "    df_feat = generate_labels_and_features(df_raw.copy(), twap_forward_period, twap_lookback_period)\n",
    "\n",
    "    # ã“ã®æ™‚ç‚¹ã§ NaN ãŒå…¥ã‚‹åˆ—ã‚’è½ã¨ã—ã¦å­¦ç¿’æ™‚ã¨åŒæ§˜ã®æ•´å½¢\n",
    "    use_cols = ['timestamp'] + features + [label_col]\n",
    "    data = df_feat[use_cols].dropna().reset_index(drop=True)\n",
    "\n",
    "    # --- 2) ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼èª­ã¿è¾¼ã¿ & æ¨™æº–åŒ–ï¼ˆç‰¹å¾´ã®ã¿ï¼‰ ---\n",
    "    scaler: StandardScaler = joblib.load(scaler_path)\n",
    "    X2d = scaler.transform(data[features].values)\n",
    "    y1d = data[label_col].astype(int).values\n",
    "\n",
    "    # --- 3) ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°çª“ï¼ˆå­¦ç¿’æ™‚ã¨æ•´åˆã®ã‚ã‚‹ä½œã‚Šï¼‰ ---\n",
    "    if len(X2d) < seq_len:\n",
    "        raise ValueError(f\"æ¨è«–ç”¨ãƒ‡ãƒ¼ã‚¿ä¸è¶³: len={len(X2d)}, SEQ_LEN={seq_len}\")\n",
    "\n",
    "    # ã“ã“ã§ã¯é‡ã¿ä¸è¦ãªã®ã§ãƒ€ãƒŸãƒ¼ã§OK\n",
    "    def make_sequences_for_infer(X2d: np.ndarray, y1d: np.ndarray, seq_len: int):\n",
    "        Xs, ys = [], []\n",
    "        for i in range(seq_len-1, len(X2d)):\n",
    "            Xs.append(X2d[i-seq_len+1:i+1])\n",
    "            ys.append(y1d[i])\n",
    "        return np.stack(Xs), np.array(ys)\n",
    "\n",
    "    X_seq, y_seq = make_sequences_for_infer(X2d, y1d, seq_len)\n",
    "    X_t = torch.tensor(X_seq, dtype=torch.float32, device=device)\n",
    "\n",
    "    # --- 4) ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ ---\n",
    "    ckpt = torch.load(model_path, map_location=device, weights_only=True)\n",
    "    params = ckpt.get(\"params\", {})\n",
    "    input_dim  = params.get(\"input_dim\", len(features))\n",
    "    hidden_dim = params.get(\"hidden\", LSTM_hidden)\n",
    "    num_layers = params.get(\"layers\", LSTM_layers)\n",
    "    dropout    = params.get(\"dropout\", LSTM_dropout)\n",
    "    num_classes= params.get(\"num_classes\", LSTM_num_classes)\n",
    "\n",
    "    model = LSTMClassifier(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        num_classes=num_classes,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "\n",
    "    # --- 5) ãƒãƒƒãƒæ¨è«–ï¼ˆç¢ºç‡ & äºˆæ¸¬ãƒ©ãƒ™ãƒ«ï¼‰ ---\n",
    "    preds = []\n",
    "    prob1 = []  # ã‚¯ãƒ©ã‚¹1(=Long) ã®ç¢ºç‡\n",
    "    for i in range(0, len(X_t), batch_size):\n",
    "        xb = X_t[i:i+batch_size]\n",
    "        logits = model(xb)                     # (B, num_classes)\n",
    "        probs  = torch.softmax(logits, dim=1)  # (B, num_classes)\n",
    "        pred   = probs.argmax(dim=1)           # (B,)\n",
    "        preds.append(pred.cpu().numpy())\n",
    "        if probs.shape[1] >= 2:\n",
    "            prob1.append(probs[:, 1].cpu().numpy())\n",
    "        else:\n",
    "            # 2ã‚¯ãƒ©ã‚¹æƒ³å®šã ãŒå¿µã®ãŸã‚\n",
    "            prob1.append(np.full(len(pred), np.nan))\n",
    "\n",
    "    preds = np.concatenate(preds)\n",
    "    prob1 = np.concatenate(prob1)\n",
    "\n",
    "    # --- 6) æ™‚ç³»åˆ—æ•´åˆï¼šäºˆæ¸¬ã¯ data ã® [seq_len-1:] ã«å¯¾å¿œ ---\n",
    "    effective = data.iloc[seq_len-1:].copy()\n",
    "    effective = effective.reset_index(drop=True)\n",
    "    assert len(effective) == len(preds)\n",
    "\n",
    "    # --- 7) å‡ºåŠ›DFæ§‹ç¯‰ï¼ˆOHLCV, Pred, Labelï¼‰ ---\n",
    "    # df_raw ã‹ã‚‰åŒã˜ timestamp ã«ä¸€è‡´ã™ã‚‹ OHLCV ã‚’ç´ã¥ã‘ã‚‹\n",
    "    # å‰æ: df_raw ã« 'timestamp','O','H','L','C','V' ãŒã‚ã‚‹\n",
    "    base = df_raw.copy()\n",
    "    base['timestamp'] = pd.to_datetime(base['timestamp'])\n",
    "    out = effective[['timestamp']].merge(\n",
    "        base[['timestamp', 'O','H','L','C','V']],\n",
    "        on='timestamp',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    out['Pred']  = preds.astype(int)\n",
    "    out['Label'] = effective[label_col].astype(int).values\n",
    "    if keep_prob:\n",
    "        out['ProbLong'] = prob1.astype(float)\n",
    "\n",
    "    # åˆ—é †ã‚’æ˜ç¤º\n",
    "    cols = ['timestamp','O','H','L','C','V','Pred','Label']\n",
    "    if keep_prob:\n",
    "        cols.append('ProbLong')\n",
    "    df_predicted = out[cols].copy()\n",
    "\n",
    "    return df_predicted\n",
    "\n",
    "\n",
    "# === å®Ÿè¡Œä¾‹ ===\n",
    "# ã™ã§ã«ä¸Šã®ã‚»ãƒ«ã§ `df_raw`, `model_path`, `scaler_path`, `Features`, `Label`, `LSTM_lookback`, `device` ãŒå®šç¾©æ¸ˆã¿ã®æƒ³å®š\n",
    "df_predicted = run_inference_build_df(\n",
    "    df_raw=df_raw,\n",
    "    model_path=model_path,\n",
    "    scaler_path=scaler_path,\n",
    "    features=Features,\n",
    "    label_col=Label,\n",
    "    seq_len=LSTM_lookback,\n",
    "    device=device,\n",
    "    batch_size=LSTM_batch,\n",
    "    keep_prob=False,          # ç¢ºä¿¡åº¦ãŒæ¬²ã—ã‘ã‚Œã° True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bf4e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#11. VWAP Regime + Position/Trade columns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def build_vwap_regime_and_trades(\n",
    "    df_pred: pd.DataFrame,\n",
    "    vwap_short: int = 20,\n",
    "    vwap_medium: int = 60,\n",
    "    price_col: str = \"C\",\n",
    "    use_typical_price: bool = False,  # Trueãªã‚‰ (H+L+C)/3 ã‚’ä½¿ç”¨\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    å…¥åŠ›: df_pred (timestamp, O,H,L,C,V, Pred ã‚’å«ã‚€)\n",
    "    å‡ºåŠ›: df_out = df_pred ã« [VWAP_short, VWAP_medium, Regeme, Position, Trade] ã‚’ä»˜ä¸\n",
    "    ä»•æ§˜:\n",
    "      1) VWAP_short/medium ã¯ Rolling(window) ã® (Price*V).sum / V.sum\n",
    "      2) Regeme: VWAP_s > VWAP_m â†’ 1 / VWAP_s < VWAP_m â†’ 0 / åŒå€¤ã‚„NaNã¯ NaN\n",
    "      3) Position:\n",
    "           - å…ˆé ­ã¯ NaN ã§é–‹å§‹\n",
    "           - Pred ã¨ Regeme ãŒåˆã‚ã¦ä¸€è‡´ã—ãŸã‚‰ã€ãã®å€¤(0/1)ã§ãƒã‚¸ã‚·ãƒ§ãƒ³é–‹å§‹\n",
    "           - ä»¥é™ Pred==Regeme ã‹ã¤ ç¾åœ¨ãƒã‚¸ã‚·ãƒ§ãƒ³ã¨ç•°ãªã‚‹ã¨ãã ã‘ 0/1 ã‚’å…¥ã‚Œæ›¿ãˆ\n",
    "           - ãã‚Œä»¥å¤–ã¯ç›´å‰ã®ãƒã‚¸ã‚·ãƒ§ãƒ³ã‚’ç¶­æŒ\n",
    "      4) Trade:\n",
    "           - åŸºæœ¬ NaN\n",
    "           - Position ãŒåˆ‡ã‚Šæ›¿ã‚ã£ãŸãƒãƒ¼ã ã‘ 0/1 ã‚’è¨˜éŒ²ï¼ˆåˆå›é–‹å§‹æ™‚ã‚‚è¨˜éŒ²ï¼‰\n",
    "    \"\"\"\n",
    "    df = df_pred.copy()\n",
    "\n",
    "    # --- ä¾¡æ ¼ç³»åˆ— ---\n",
    "    if use_typical_price:\n",
    "        price = (df['O'] + df[\"H\"] + df[\"L\"] + df[\"C\"]) / 4.0\n",
    "    else:\n",
    "        price = df[price_col].astype(float)\n",
    "\n",
    "    vol = df[\"V\"].astype(float)\n",
    "\n",
    "    # --- Rolling VWAP ---\n",
    "    pv = price * vol\n",
    "    vwap_s = pv.rolling(vwap_short, min_periods=1).sum() / vol.rolling(vwap_short, min_periods=1).sum()\n",
    "    vwap_m = pv.rolling(vwap_medium, min_periods=1).sum() / vol.rolling(vwap_medium, min_periods=1).sum()\n",
    "\n",
    "    # å…ˆé ­ã®å®‰å®šåŒ–: çª“ãŒåŸ‹ã¾ã£ã¦ãªã„æœŸé–“ã¯ NaN ã«ã—ãŸã„ãªã‚‰ä¸‹è¨˜ã‚’æœ‰åŠ¹åŒ–\n",
    "    vwap_s = vwap_s.where(vol.rolling(vwap_short).count() >= vwap_short, np.nan)\n",
    "    vwap_m = vwap_m.where(vol.rolling(vwap_medium).count() >= vwap_medium, np.nan)\n",
    "\n",
    "    df[\"VWAP_short\"]  = vwap_s\n",
    "    df[\"VWAP_medium\"] = vwap_m\n",
    "\n",
    "    # --- Regeme: 1/0/NaN (åŒå€¤ã¯NaNã«ã—ã¦ã‚¹ã‚¤ãƒƒãƒæŠ‘åˆ¶) ---\n",
    "    reg = np.where(\n",
    "        (df[\"VWAP_short\"] > df[\"VWAP_medium\"]),\n",
    "        1,\n",
    "        np.where(\n",
    "            (df[\"VWAP_short\"] < df[\"VWAP_medium\"]),\n",
    "            0,\n",
    "            np.nan\n",
    "        )\n",
    "    )\n",
    "    df[\"Regeme\"] = reg.astype(float)  # NaNã‚’å«ã‚€ãŸã‚ float å‹\n",
    "\n",
    "    # --- Position ã®æ§‹ç¯‰ï¼ˆçŠ¶æ…‹æ©Ÿæ¢°ï¼‰ ---\n",
    "    n = len(df)\n",
    "    pos = np.full(n, np.nan, dtype=float)\n",
    "\n",
    "    have_started = False\n",
    "    current = np.nan\n",
    "\n",
    "    pred = df[\"Pred\"].to_numpy(dtype=float)  # 0/1 æƒ³å®šï¼ˆNaNã‚‚è¨±å®¹ï¼‰\n",
    "    rege = df[\"Regeme\"].to_numpy(dtype=float)\n",
    "\n",
    "    for i in range(n):\n",
    "        p = pred[i]\n",
    "        r = rege[i]\n",
    "\n",
    "        # ã©ã¡ã‚‰ã‹ãŒ NaN â†’ ã‚¹ã‚¤ãƒƒãƒã—ãªã„\n",
    "        if np.isnan(p) or np.isnan(r):\n",
    "            if have_started:\n",
    "                pos[i] = current\n",
    "            else:\n",
    "                pos[i] = np.nan\n",
    "            continue\n",
    "\n",
    "        # ã¾ã é–‹å§‹ã—ã¦ã„ãªã„ â†’ ä¸€è‡´ã—ãŸã‚‰é–‹å§‹\n",
    "        if not have_started:\n",
    "            if p == r:\n",
    "                current = p\n",
    "                have_started = True\n",
    "                pos[i] = current\n",
    "            else:\n",
    "                pos[i] = np.nan\n",
    "            continue\n",
    "\n",
    "        # æ—¢ã«é–‹å§‹æ¸ˆã¿ â†’ ä¸€è‡´ã—ã¦ã€ã‹ã¤ç¾åœ¨ã¨é•ãˆã°åˆ‡æ›¿\n",
    "        if p == r and current != p:\n",
    "            current = p\n",
    "\n",
    "        pos[i] = current\n",
    "\n",
    "    df[\"Position\"] = pos  # 0/1/NaN\n",
    "\n",
    "    # --- Trade: Position ãŒåˆ‡ã‚Šæ›¿ã‚ã‚‹ãƒã‚¤ãƒ³ãƒˆã ã‘ 0/1ã€ãã‚Œä»¥å¤– NaN ---\n",
    "    # å…ˆé ­é–‹å§‹ã‚‚ã€Œåˆ‡æ›¿ã€ã¨ã¿ãªã™\n",
    "    position_series = pd.Series(df[\"Position\"])\n",
    "    changed = position_series.ne(position_series.shift(1))  # å¤‰æ›´ãƒ•ãƒ©ã‚°ï¼ˆNaNæ¯”è¼ƒã‚‚Trueã«ãªã‚‹ï¼‰\n",
    "    # ãŸã ã—ã€Œä¸¡æ–¹NaNã€ã®ã¨ãã¯å¤‰æ›´ã§ã¯ãªã„ã®ã§è£œæ­£\n",
    "    both_nan = position_series.isna() & position_series.shift(1).isna()\n",
    "    changed = changed & ~both_nan\n",
    "\n",
    "    trade = position_series.where(changed, np.nan)\n",
    "    df[\"Trade\"] = trade\n",
    "\n",
    "    # åˆ—é †ã‚’å°‘ã—æ•´ãˆã¦è¿”ã™ï¼ˆä»»æ„ï¼‰\n",
    "    desired_order = [\n",
    "        \"timestamp\", \"O\",\"H\",\"L\",\"C\",\"V\",\n",
    "        \"Pred\", \"Label\" if \"Label\" in df.columns else None,\n",
    "        \"VWAP_short\",\"VWAP_medium\",\"Regeme\",\"Position\",\"Trade\"\n",
    "    ]\n",
    "    desired_order = [c for c in desired_order if c is not None]\n",
    "    df = df[desired_order]\n",
    "\n",
    "    return df\n",
    "\n",
    "# === ä½¿ã„æ–¹ä¾‹ ===\n",
    "# df_rule = build_vwap_regime_and_trades(df_pred, vwap_short=20, vwap_medium=60, price_col=\"C\", use_typical_price=False)\n",
    "# display(df_rule.head(15))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
