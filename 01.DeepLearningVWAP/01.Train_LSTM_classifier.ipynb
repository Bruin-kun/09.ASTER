{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd9595ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Saved scaler: ./scaler_ASTERUSDT_1min.gz\n",
      "shapes ‚Üí X_train (6603, 48, 5) y_train (6603,) w_train (6603,) | X_val (692, 48, 5) y_val (692,) w_val (692,) | X_test (1801, 48, 5) y_test (1801,) w_test (1801,)\n",
      "[001/50] train_loss 0.6462 | val_loss 0.5588 | val_acc 0.717 | val_bal_acc 0.702 | val_auc 0.768 | best_metric 0.7024 | no_improve 0/8\n",
      "[005/50] train_loss 0.5442 | val_loss 0.5199 | val_acc 0.718 | val_bal_acc 0.714 | val_auc 0.797 | best_metric 0.7209 | no_improve 1/8\n",
      "[010/50] train_loss 0.5337 | val_loss 0.5082 | val_acc 0.754 | val_bal_acc 0.732 | val_auc 0.802 | best_metric 0.7319 | no_improve 0/8\n",
      "[015/50] train_loss 0.5320 | val_loss 0.5135 | val_acc 0.730 | val_bal_acc 0.727 | val_auc 0.801 | best_metric 0.7319 | no_improve 5/8\n",
      "‚èπÔ∏è Early stopped at epoch 18 (best at 10, best_metric=0.7319).\n",
      "‚úÖ Restored best validation model (epoch 10, best_metric=0.7319).\n",
      "üíæ Model saved to ./Models\\LSTM_classify_ASTERUSDT_2509292209.pt\n",
      "\n",
      "[Test] loss 0.6401 | acc 0.670 | bal_acc 0.680 | auc 0.775\n",
      "\n",
      "Classification report (labels: 0=Short, 1=Long):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Short(0)      0.760     0.597     0.669      1004\n",
      "     Long(1)      0.600     0.763     0.672       797\n",
      "\n",
      "    accuracy                          0.670      1801\n",
      "   macro avg      0.680     0.680     0.670      1801\n",
      "weighted avg      0.689     0.670     0.670      1801\n",
      "\n",
      "Confusion matrix:\n",
      " [[599 405]\n",
      " [189 608]]\n"
     ]
    }
   ],
   "source": [
    "#0.Import Libraries & Basic Settings\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"torch\")\n",
    "\n",
    "import os, datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# Êó¢Â≠ò„É¢„Ç∏„É•„Éº„É´\n",
    "from Generate_Features_and_Labels import generate_labels_and_features, add_trade_weights\n",
    "from LSTM_model_classify import LSTMClassifier, make_latest_lstm_window\n",
    "\n",
    "#1.Set Parameters\n",
    "# „Éá„Éº„Çø\n",
    "symbol = 'ASTERUSDT'\n",
    "csv_filename = 'Market_Bybit_ASTERUSDT_1min_20250901-20250926'\n",
    "csv_path = f'01.data/{csv_filename}.csv'\n",
    "model_min = 1\n",
    "\n",
    "# „É©„Éô„É´ÁîüÊàê\n",
    "twap_forward_period = 60\n",
    "twap_lookback_period = 60\n",
    "\n",
    "# ÊêçÂ§±„Ç¶„Çß„Ç§„ÉàË®≠ÂÆö\n",
    "weight_fee = 0.0011\n",
    "weight_slip = 0.000\n",
    "weight_quantile = 0.95\n",
    "weight_gamma = 0.9995\n",
    "weight_alpha = 0.9\n",
    "weight_min = 0.0\n",
    "weitght_max = 3.0\n",
    "weight_sigma_scale = 0.1\n",
    "\n",
    "# LSTM\n",
    "LSTM_lookback   = 48\n",
    "LSTM_hidden     = 128\n",
    "LSTM_layers     = 2\n",
    "LSTM_dropout    = 0.2\n",
    "LSTM_num_classes= 2\n",
    "LSTM_learnRate  = 1e-3\n",
    "LSTM_gamma      = 0.985\n",
    "LSTM_epochs     = 50\n",
    "LSTM_train_ratio= 0.8\n",
    "LSTM_val_frac   = 0.1\n",
    "LSTM_patience   = 8\n",
    "LSTM_min_delta  = 1e-3\n",
    "LSTM_batch = 256\n",
    "LSTM_seed = 18\n",
    "\n",
    "# ÁâπÂæ¥/„É©„Éô„É´\n",
    "Features = ['O_log', 'H_log', 'L_log', 'C_log', 'V_log']\n",
    "Label    = 'label'\n",
    "\n",
    "# ‰øùÂ≠ò\n",
    "models_dir  = \"./Models\"\n",
    "scaler_path = f'./scaler_{symbol}_{model_min}min.gz'   # ‚òÖË∂≥„Å´Âêà„Çè„Åõ„Å¶ÂëΩÂêç\n",
    "batch_size  = LSTM_batch\n",
    "seed        = LSTM_seed\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === SeedÂõ∫ÂÆö ===\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "#3.Utility Functions (only within ipynb)\n",
    "def f3(x):\n",
    "    try:\n",
    "        if x is None: return \"nan\"\n",
    "        if isinstance(x, float) and (np.isnan(x) or np.isinf(x)): return \"nan\"\n",
    "        return f\"{x:.3f}\"\n",
    "    except Exception:\n",
    "        return \"nan\"\n",
    "\n",
    "def make_sequences(X2d: np.ndarray, y1d: np.ndarray, w1d: np.ndarray, seq_len: int):\n",
    "    if len(X2d) < seq_len:\n",
    "        raise ValueError(f\"„Éá„Éº„Çø‰∏çË∂≥: len={len(X2d)}, SEQ_LEN={seq_len}\")\n",
    "    Xs, ys, ws = [], [], []\n",
    "    for i in range(seq_len-1, len(X2d)):\n",
    "        Xs.append(X2d[i-seq_len+1:i+1])\n",
    "        ys.append(y1d[i])\n",
    "        ws.append(w1d[i])   # „É©„Éô„É´„Å®Âêå„ÅòÊôÇÁÇπ„ÅÆÈáç„Åø„Çí‰ªò‰∏é\n",
    "    return np.stack(Xs), np.array(ys), np.array(ws)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, device, criterion=None, weighted=False):\n",
    "    \"\"\"\n",
    "    loader: (X,y) „Åæ„Åü„ÅØ (X,y,w) „ÅÆ„ÅÑ„Åö„Çå„Åã\n",
    "    criterion:\n",
    "      - None „Å™„ÇâÊêçÂ§±„ÅØË®àÁÆó„Åó„Å™„ÅÑ\n",
    "      - reduction='none' „ÅÆCE„ÇíÊ∏°„Åô„Å® weighted=True „ÅßÈáç„Åø‰ªò„ÅçÊêçÂ§±„ÇíË®àÁÆó\n",
    "      - „Åù„Çå‰ª•Â§ñ„Å™„ÇâÈÄöÂ∏∏Âπ≥ÂùáÊêçÂ§±\n",
    "    weighted: True „ÅÆ„Å®„Åç (X,y,w) „Å™„ÇâÈáç„Åø‰ªò„ÅçÂπ≥ÂùáÊêçÂ§±„ÅßÈõÜË®à\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_y, all_pred, all_prob = [], [], []\n",
    "    total_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            if len(batch) == 3:\n",
    "                Xb, yb, wb = batch\n",
    "                wb = wb.to(device)\n",
    "            else:\n",
    "                Xb, yb = batch\n",
    "                wb = None\n",
    "\n",
    "            Xb = Xb.to(device); yb = yb.to(device)\n",
    "            logits = model(Xb)\n",
    "\n",
    "            # --- ÊêçÂ§± ---\n",
    "            if criterion is not None:\n",
    "                if weighted and wb is not None and getattr(criterion, 'reduction', 'mean') == 'none':\n",
    "                    loss_vec = criterion(logits, yb)                 # (B,)\n",
    "                    denom = torch.clamp(wb.sum(), min=1e-8)\n",
    "                    loss = (loss_vec * wb).sum() / denom\n",
    "                else:\n",
    "                    # ÈÄöÂ∏∏ÔºàÈáç„ÅøÁÑ°Ë¶ñÔºâ„ÅÆÂπ≥ÂùáÊêçÂ§±\n",
    "                    loss = criterion(logits, yb)\n",
    "                total_loss += float(loss.detach().cpu())\n",
    "                n_batches += 1\n",
    "\n",
    "            # --- ‰∫àÊ∏¨/„É°„Éà„É™„ÇØ„ÇπÁî® ---\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            pred  = probs.argmax(dim=1)\n",
    "\n",
    "            all_y.append(yb.cpu()); all_pred.append(pred.cpu()); all_prob.append(probs.cpu())\n",
    "\n",
    "    all_y    = torch.cat(all_y).numpy()\n",
    "    all_pred = torch.cat(all_pred).numpy()\n",
    "    all_prob = torch.cat(all_prob).numpy()\n",
    "\n",
    "    acc = (all_y == all_pred).mean()\n",
    "    bal_acc, auc = float('nan'), float('nan')\n",
    "    if len(np.unique(all_y)) >= 2:\n",
    "        try:\n",
    "            bal_acc = balanced_accuracy_score(all_y, all_pred)\n",
    "        except: pass\n",
    "        try:\n",
    "            if all_prob.shape[1] == 2:\n",
    "                auc = roc_auc_score(all_y, all_prob[:, 1])\n",
    "            else:\n",
    "                auc = roc_auc_score(all_y, all_prob, multi_class=\"ovr\")\n",
    "        except: pass\n",
    "\n",
    "    avg_loss = total_loss / max(1, n_batches) if criterion is not None else float('nan')\n",
    "    return avg_loss, acc, bal_acc, auc, all_y, all_pred, all_prob\n",
    "\n",
    "\n",
    "#4.Read csv and generate features/labels\n",
    "df_raw = pd.read_csv(csv_path)\n",
    "df_raw['timestamp'] = pd.to_datetime(df_raw['timestamp'])\n",
    "\n",
    "df_feat = generate_labels_and_features(df_raw, twap_forward_period, twap_lookback_period)\n",
    "df_w = add_trade_weights(df_feat, fee=weight_fee, slip=weight_slip, rem_horizon=twap_forward_period, quantile=weight_quantile,\n",
    "                         gamma=weight_gamma, alpha=weight_alpha, wmin=weight_min, wmax=weitght_max, sigma_scale=weight_sigma_scale)\n",
    "\n",
    "use_cols = ['timestamp'] + Features + [Label, 'weight', 'weight_base', 'weight_rem']\n",
    "data = df_w[use_cols].dropna().reset_index(drop=True)\n",
    "data.head()\n",
    "\n",
    "#5.Split / Standardize & Sequence\n",
    "split_idx = int(len(data) * LSTM_train_ratio)\n",
    "train_all = data.iloc[:split_idx].copy()\n",
    "test_df   = data.iloc[split_idx:].copy()\n",
    "val_idx   = int(len(train_all) * (1 - LSTM_val_frac))\n",
    "train_df  = train_all.iloc[:val_idx].copy()\n",
    "val_df    = train_all.iloc[val_idx:].copy()\n",
    "\n",
    "# StandardizeÔºàÁâπÂæ¥„Å†„ÅëÔºâ\n",
    "scaler = StandardScaler()\n",
    "X_train_2d = scaler.fit_transform(train_df[Features].values)\n",
    "X_val_2d   = scaler.transform(val_df[Features].values)\n",
    "X_test_2d  = scaler.transform(test_df[Features].values)\n",
    "\n",
    "y_train_1d = train_df[Label].astype(int).values\n",
    "y_val_1d   = val_df[Label].astype(int).values\n",
    "y_test_1d  = test_df[Label].astype(int).values\n",
    "\n",
    "w_train_1d = train_df['weight'].astype(float).values\n",
    "w_val_1d   = val_df['weight'].astype(float).values\n",
    "w_test_1d  = test_df['weight'].astype(float).values\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(\"Saved scaler:\", scaler_path)\n",
    "\n",
    "# SequencesÔºàÈáç„ÅøËæº„ÅøÔºâ\n",
    "X_train, y_train, w_train = make_sequences(X_train_2d, y_train_1d, w_train_1d, LSTM_lookback)\n",
    "X_val,   y_val,   w_val   = make_sequences(X_val_2d,   y_val_1d,   w_val_1d,   LSTM_lookback)\n",
    "X_test,  y_test,  w_test  = make_sequences(X_test_2d,  y_test_1d,  w_test_1d,  LSTM_lookback)\n",
    "\n",
    "print(\"shapes ‚Üí\",\n",
    "      \"X_train\", X_train.shape, \"y_train\", y_train.shape, \"w_train\", w_train.shape,\n",
    "      \"| X_val\", X_val.shape,   \"y_val\",   y_val.shape,   \"w_val\",   w_val.shape,\n",
    "      \"| X_test\", X_test.shape, \"y_test\",  y_test.shape,  \"w_test\",  w_test.shape)\n",
    "\n",
    "\n",
    "#6.Tensor/Dataloader\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "w_train_t = torch.tensor(w_train, dtype=torch.float32)\n",
    "\n",
    "X_val_t = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_t = torch.tensor(y_val, dtype=torch.long)\n",
    "w_val_t = torch.tensor(w_val, dtype=torch.float32)\n",
    "\n",
    "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_t = torch.tensor(y_test, dtype=torch.long)\n",
    "w_test_t = torch.tensor(w_test, dtype=torch.float32)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_t, y_train_t, w_train_t), batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(TensorDataset(X_val_t,   y_val_t,   w_val_t),   batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(TensorDataset(X_test_t,  y_test_t,  w_test_t),  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "#7.Model\n",
    "model = LSTMClassifier(\n",
    "    input_dim=len(Features),\n",
    "    hidden_dim=LSTM_hidden,\n",
    "    num_layers=LSTM_layers,\n",
    "    num_classes=LSTM_num_classes,\n",
    "    dropout=LSTM_dropout\n",
    ").to(device)\n",
    "\n",
    "criterion_ce = nn.CrossEntropyLoss(reduction='none')\n",
    "optimizer = optim.Adam(model.parameters(), lr=LSTM_learnRate)\n",
    "scheduler = ExponentialLR(optimizer, gamma=LSTM_gamma)\n",
    "\n",
    "#8.Training\n",
    "patience  = LSTM_patience\n",
    "min_delta = LSTM_min_delta\n",
    "no_improve = 0\n",
    "best_val = None\n",
    "best_state = None\n",
    "best_epoch = None\n",
    "\n",
    "def select_metric(val_balacc, val_acc):\n",
    "    m = val_balacc\n",
    "    if isinstance(m, float) and (np.isnan(m) or np.isinf(m)):\n",
    "        m = val_acc\n",
    "    return float(m)\n",
    "\n",
    "use_amp = (device.type == \"cuda\")\n",
    "scaler_amp = torch.amp.GradScaler(\"cuda\") if use_amp else None\n",
    "\n",
    "for epoch in range(1, LSTM_epochs + 1):\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        Xb, yb, wb = batch\n",
    "        Xb = Xb.to(device); yb = yb.to(device); wb = wb.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if use_amp:\n",
    "            with torch.amp.autocast(\"cuda\"):\n",
    "                logits = model(Xb)                             # (B, 2)\n",
    "                loss_vec = criterion_ce(logits, yb)           # (B,)\n",
    "                # Ê≠£Ë¶èÂåñÔºöÈáç„Åø„ÅÆÂíå„ÅßÂâ≤„ÇãÔºà„Çº„É≠ÂàÜÊØçÂõûÈÅøÔºâ\n",
    "                denom = torch.clamp(wb.sum(), min=1e-8)\n",
    "                loss = (loss_vec * wb).sum() / denom\n",
    "            scaler_amp.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler_amp.step(optimizer)\n",
    "            scaler_amp.update()\n",
    "        else:\n",
    "            logits = model(Xb)\n",
    "            loss_vec = criterion_ce(logits, yb)\n",
    "            denom = torch.clamp(wb.sum(), min=1e-8)\n",
    "            loss = (loss_vec * wb).sum() / denom\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        total += float(loss.detach().cpu())\n",
    "\n",
    "    train_loss = total / max(1, len(train_loader))\n",
    "    scheduler.step()\n",
    "\n",
    "    criterion_eval = nn.CrossEntropyLoss(reduction='none')\n",
    "    val_loss, val_acc, val_balacc, val_auc, *_ = evaluate(model, val_loader, device, criterion_eval, weighted=True)\n",
    "\n",
    "    metric = select_metric(val_balacc, val_acc)\n",
    "\n",
    "    improved = (best_val is None) or (metric > best_val + min_delta)\n",
    "\n",
    "\n",
    "    if improved:\n",
    "        best_val = metric\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        best_epoch = epoch\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "\n",
    "    if epoch % 5 == 0 or epoch == 1:\n",
    "        print(f\"[{epoch:03d}/{LSTM_epochs}] \"\n",
    "              f\"train_loss {train_loss:.4f} | \"\n",
    "              f\"val_loss {val_loss:.4f} | \"\n",
    "              f\"val_acc {f3(val_acc)} | \"\n",
    "              f\"val_bal_acc {f3(val_balacc)} | \"\n",
    "              f\"val_auc {f3(val_auc)} | \"\n",
    "              f\"best_metric {best_val:.4f} | \"\n",
    "              f\"no_improve {no_improve}/{patience}\")\n",
    "\n",
    "    if no_improve >= patience:\n",
    "        print(f\"‚èπÔ∏è Early stopped at epoch {epoch} (best at {best_epoch}, best_metric={best_val:.4f}).\")\n",
    "        break\n",
    "\n",
    "\n",
    "# === Best restoreÔºàÂêå„ÅòÔºâ ===\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "    print(f\"‚úÖ Restored best validation model (epoch {best_epoch}, best_metric={best_val:.4f}).\")\n",
    "\n",
    "# === Save path: Êú™ÂÆöÁæ©Â§âÊï∞„Çí‰Ωø„Çè„Å™„ÅÑÂëΩÂêç„Å´‰øÆÊ≠£ ===\n",
    "ts = datetime.datetime.now().strftime(\"%y%m%d%H%M\")\n",
    "# ‰æã1: „ÅîÂ∏åÊúõ„ÅÆÊúÄÂ∞èÂΩ¢Âºè\n",
    "model_path = os.path.join(models_dir, f\"LSTM_classify_{symbol}_{ts}.pt\")\n",
    "# ‰æã2: „ÇÇ„ÅÜÂ∞ë„ÅóÊÉÖÂ†±„ÇíÊåÅ„Åü„Åõ„Çã„Å™„ÇâÔºà‰ªªÊÑèÔºâ\n",
    "# model_path = os.path.join(models_dir, f\"LSTM_classify_{symbol}_{model_min}m_lb{LSTM_lookback}_{ts}.pt\")\n",
    "\n",
    "torch.save({\n",
    "    \"epoch\": best_epoch,\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"best_metric\": best_val,\n",
    "    \"params\": {\n",
    "        \"lookback\": LSTM_lookback,\n",
    "        \"hidden\": LSTM_hidden,\n",
    "        \"layers\": LSTM_layers,\n",
    "        \"dropout\": LSTM_dropout,\n",
    "        \"num_classes\": LSTM_num_classes,\n",
    "        \"input_dim\": len(Features),\n",
    "        \"features\": Features,           # ‚òÖËøΩÂä†Ôºà‰ªªÊÑèÔºâ\n",
    "        \"scaler_path\": scaler_path,     # ‚òÖËøΩÂä†Ôºà‰ªªÊÑèÔºâ\n",
    "        \"train_ratio\": LSTM_train_ratio,# ‚òÖËøΩÂä†Ôºà‰ªªÊÑèÔºâ\n",
    "    }\n",
    "}, model_path)\n",
    "print(f\"üíæ Model saved to {model_path}\")\n",
    "\n",
    "#9.Model Evaluation\n",
    "criterion_eval = nn.CrossEntropyLoss()  # ‚òÖË©ï‰æ°Áî®„Å´ËøΩÂä†\n",
    "\n",
    "criterion_eval = nn.CrossEntropyLoss(reduction='none')\n",
    "test_loss, test_acc, test_balacc, test_auc, y_true, y_pred, y_prob = evaluate(model, test_loader, device, criterion_eval, weighted=True)\n",
    "\n",
    "print(f\"\\n[Test] loss {test_loss:.4f} | acc {f3(test_acc)} | bal_acc {f3(test_balacc)} | auc {f3(test_auc)}\")\n",
    "\n",
    "print(\"\\nClassification report (labels: 0=Short, 1=Long):\")\n",
    "print(classification_report(y_true, y_pred, digits=3, target_names=[\"Short(0)\", \"Long(1)\"]))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e65d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10. Inference / Build df_predicted\n",
    "import torch\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_inference_build_df(\n",
    "    df_raw: pd.DataFrame,\n",
    "    model_path: str,\n",
    "    scaler_path: str,\n",
    "    features: list,\n",
    "    label_col: str,\n",
    "    seq_len: int,\n",
    "    device: Optional[torch.device] = None,\n",
    "    batch_size: int = 1024,\n",
    "    keep_prob: bool = False,   # True„Å™„ÇâÁ¢∫‰ø°Â∫¶„ÇÇÂá∫Âäõ\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Â≠¶ÁøíÊ∏à„ÅøLSTMÂàÜÈ°û„É¢„Éá„É´„Çí„É≠„Éº„Éâ„Åó„ÄÅdf_rawÂÖ®‰Ωì„Å´ÂØæ„Åó„Å¶„Çπ„É©„Ç§„Éá„Ç£„É≥„Ç∞Êé®Ë´ñ„ÄÇ\n",
    "    Ëøî„ÇäÂÄ§„ÅØ [timestamp, O,H,L,C,V, Pred, Label] (+ ProbLong ‰ªªÊÑè) „ÇíÊåÅ„Å§ df_predicted„ÄÇ\n",
    "    \"\"\"\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # --- 1) ÁâπÂæ¥„Éª„É©„Éô„É´ÂÜçÁîüÊàêÔºàÂ≠¶ÁøíÊôÇ„Å®Âêå„ÅòÈñ¢Êï∞„ÇíÂà©Áî®Ôºâ ---\n",
    "    df_feat = generate_labels_and_features(df_raw.copy(), twap_forward_period, twap_lookback_period)\n",
    "\n",
    "    # „Åì„ÅÆÊôÇÁÇπ„Åß NaN „ÅåÂÖ•„ÇãÂàó„ÇíËêΩ„Å®„Åó„Å¶Â≠¶ÁøíÊôÇ„Å®ÂêåÊßò„ÅÆÊï¥ÂΩ¢\n",
    "    use_cols = ['timestamp'] + features + [label_col]\n",
    "    data = df_feat[use_cols].dropna().reset_index(drop=True)\n",
    "\n",
    "    # --- 2) „Çπ„Ç±„Éº„É©„ÉºË™≠„ÅøËæº„Åø & Ê®ôÊ∫ñÂåñÔºàÁâπÂæ¥„ÅÆ„ÅøÔºâ ---\n",
    "    scaler: StandardScaler = joblib.load(scaler_path)\n",
    "    X2d = scaler.transform(data[features].values)\n",
    "    y1d = data[label_col].astype(int).values\n",
    "\n",
    "    # --- 3) „Çπ„É©„Ç§„Éá„Ç£„É≥„Ç∞Á™ìÔºàÂ≠¶ÁøíÊôÇ„Å®Êï¥Âêà„ÅÆ„ÅÇ„Çã‰Ωú„ÇäÔºâ ---\n",
    "    if len(X2d) < seq_len:\n",
    "        raise ValueError(f\"Êé®Ë´ñÁî®„Éá„Éº„Çø‰∏çË∂≥: len={len(X2d)}, SEQ_LEN={seq_len}\")\n",
    "\n",
    "    # „Åì„Åì„Åß„ÅØÈáç„Åø‰∏çË¶Å„Å™„ÅÆ„Åß„ÉÄ„Éü„Éº„ÅßOK\n",
    "    def make_sequences_for_infer(X2d: np.ndarray, y1d: np.ndarray, seq_len: int):\n",
    "        Xs, ys = [], []\n",
    "        for i in range(seq_len-1, len(X2d)):\n",
    "            Xs.append(X2d[i-seq_len+1:i+1])\n",
    "            ys.append(y1d[i])\n",
    "        return np.stack(Xs), np.array(ys)\n",
    "\n",
    "    X_seq, y_seq = make_sequences_for_infer(X2d, y1d, seq_len)\n",
    "    X_t = torch.tensor(X_seq, dtype=torch.float32, device=device)\n",
    "\n",
    "    # --- 4) „É¢„Éá„É´Ë™≠„ÅøËæº„Åø ---\n",
    "    ckpt = torch.load(model_path, map_location=device, weights_only=True)\n",
    "    params = ckpt.get(\"params\", {})\n",
    "    input_dim  = params.get(\"input_dim\", len(features))\n",
    "    hidden_dim = params.get(\"hidden\", LSTM_hidden)\n",
    "    num_layers = params.get(\"layers\", LSTM_layers)\n",
    "    dropout    = params.get(\"dropout\", LSTM_dropout)\n",
    "    num_classes= params.get(\"num_classes\", LSTM_num_classes)\n",
    "\n",
    "    model = LSTMClassifier(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        num_classes=num_classes,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "\n",
    "    # --- 5) „Éê„ÉÉ„ÉÅÊé®Ë´ñÔºàÁ¢∫Áéá & ‰∫àÊ∏¨„É©„Éô„É´Ôºâ ---\n",
    "    preds = []\n",
    "    prob1 = []  # „ÇØ„É©„Çπ1(=Long) „ÅÆÁ¢∫Áéá\n",
    "    for i in range(0, len(X_t), batch_size):\n",
    "        xb = X_t[i:i+batch_size]\n",
    "        logits = model(xb)                     # (B, num_classes)\n",
    "        probs  = torch.softmax(logits, dim=1)  # (B, num_classes)\n",
    "        pred   = probs.argmax(dim=1)           # (B,)\n",
    "        preds.append(pred.cpu().numpy())\n",
    "        if probs.shape[1] >= 2:\n",
    "            prob1.append(probs[:, 1].cpu().numpy())\n",
    "        else:\n",
    "            # 2„ÇØ„É©„ÇπÊÉ≥ÂÆö„Å†„ÅåÂøµ„ÅÆ„Åü„ÇÅ\n",
    "            prob1.append(np.full(len(pred), np.nan))\n",
    "\n",
    "    preds = np.concatenate(preds)\n",
    "    prob1 = np.concatenate(prob1)\n",
    "\n",
    "    # --- 6) ÊôÇÁ≥ªÂàóÊï¥ÂêàÔºö‰∫àÊ∏¨„ÅØ data „ÅÆ [seq_len-1:] „Å´ÂØæÂøú ---\n",
    "    effective = data.iloc[seq_len-1:].copy()\n",
    "    effective = effective.reset_index(drop=True)\n",
    "    assert len(effective) == len(preds)\n",
    "\n",
    "    # --- 7) Âá∫ÂäõDFÊßãÁØâÔºàOHLCV, Pred, LabelÔºâ ---\n",
    "    # df_raw „Åã„ÇâÂêå„Åò timestamp „Å´‰∏ÄËá¥„Åô„Çã OHLCV „ÇíÁ¥ê„Å•„Åë„Çã\n",
    "    # ÂâçÊèê: df_raw „Å´ 'timestamp','O','H','L','C','V' „Åå„ÅÇ„Çã\n",
    "    base = df_raw.copy()\n",
    "    base['timestamp'] = pd.to_datetime(base['timestamp'])\n",
    "    out = effective[['timestamp']].merge(\n",
    "        base[['timestamp', 'O','H','L','C','V']],\n",
    "        on='timestamp',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    out['Pred']  = preds.astype(int)\n",
    "    out['Label'] = effective[label_col].astype(int).values\n",
    "    if keep_prob:\n",
    "        out['ProbLong'] = prob1.astype(float)\n",
    "\n",
    "    # ÂàóÈ†Ü„ÇíÊòéÁ§∫\n",
    "    cols = ['timestamp','O','H','L','C','V','Pred','Label']\n",
    "    if keep_prob:\n",
    "        cols.append('ProbLong')\n",
    "    df_predicted = out[cols].copy()\n",
    "\n",
    "    return df_predicted\n",
    "\n",
    "\n",
    "# === ÂÆüË°å‰æã ===\n",
    "# „Åô„Åß„Å´‰∏ä„ÅÆ„Çª„É´„Åß `df_raw`, `model_path`, `scaler_path`, `Features`, `Label`, `LSTM_lookback`, `device` „ÅåÂÆöÁæ©Ê∏à„Åø„ÅÆÊÉ≥ÂÆö\n",
    "df_predicted = run_inference_build_df(\n",
    "    df_raw=df_raw,\n",
    "    model_path=model_path,\n",
    "    scaler_path=scaler_path,\n",
    "    features=Features,\n",
    "    label_col=Label,\n",
    "    seq_len=LSTM_lookback,\n",
    "    device=device,\n",
    "    batch_size=LSTM_batch,\n",
    "    keep_prob=False,          # Á¢∫‰ø°Â∫¶„ÅåÊ¨≤„Åó„Åë„Çå„Å∞ True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bf4e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#11. VWAP Regime + Position/Trade columns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def build_vwap_regime_and_trades(\n",
    "    df_pred: pd.DataFrame,\n",
    "    vwap_short: int = 20,\n",
    "    vwap_medium: int = 60,\n",
    "    price_col: str = \"C\",\n",
    "    use_typical_price: bool = False,  # True„Å™„Çâ (H+L+C)/3 „Çí‰ΩøÁî®\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ÂÖ•Âäõ: df_pred (timestamp, O,H,L,C,V, Pred „ÇíÂê´„ÇÄ)\n",
    "    Âá∫Âäõ: df_out = df_pred „Å´ [VWAP_short, VWAP_medium, Regeme, Position, Trade] „Çí‰ªò‰∏é\n",
    "    ‰ªïÊßò:\n",
    "      1) VWAP_short/medium „ÅØ Rolling(window) „ÅÆ (Price*V).sum / V.sum\n",
    "      2) Regeme: VWAP_s > VWAP_m ‚Üí 1 / VWAP_s < VWAP_m ‚Üí 0 / ÂêåÂÄ§„ÇÑNaN„ÅØ NaN\n",
    "      3) Position:\n",
    "           - ÂÖàÈ†≠„ÅØ NaN „ÅßÈñãÂßã\n",
    "           - Pred „Å® Regeme „ÅåÂàù„ÇÅ„Å¶‰∏ÄËá¥„Åó„Åü„Çâ„ÄÅ„Åù„ÅÆÂÄ§(0/1)„Åß„Éù„Ç∏„Ç∑„Éß„É≥ÈñãÂßã\n",
    "           - ‰ª•Èôç Pred==Regeme „Åã„Å§ ÁèæÂú®„Éù„Ç∏„Ç∑„Éß„É≥„Å®Áï∞„Å™„Çã„Å®„Åç„Å†„Åë 0/1 „ÇíÂÖ•„ÇåÊõø„Åà\n",
    "           - „Åù„Çå‰ª•Â§ñ„ÅØÁõ¥Ââç„ÅÆ„Éù„Ç∏„Ç∑„Éß„É≥„ÇíÁ∂≠ÊåÅ\n",
    "      4) Trade:\n",
    "           - Âü∫Êú¨ NaN\n",
    "           - Position „ÅåÂàá„ÇäÊõø„Çè„Å£„Åü„Éê„Éº„Å†„Åë 0/1 „ÇíË®òÈå≤ÔºàÂàùÂõûÈñãÂßãÊôÇ„ÇÇË®òÈå≤Ôºâ\n",
    "    \"\"\"\n",
    "    df = df_pred.copy()\n",
    "\n",
    "    # --- ‰æ°Ê†ºÁ≥ªÂàó ---\n",
    "    if use_typical_price:\n",
    "        price = (df['O'] + df[\"H\"] + df[\"L\"] + df[\"C\"]) / 4.0\n",
    "    else:\n",
    "        price = df[price_col].astype(float)\n",
    "\n",
    "    vol = df[\"V\"].astype(float)\n",
    "\n",
    "    # --- Rolling VWAP ---\n",
    "    pv = price * vol\n",
    "    vwap_s = pv.rolling(vwap_short, min_periods=1).sum() / vol.rolling(vwap_short, min_periods=1).sum()\n",
    "    vwap_m = pv.rolling(vwap_medium, min_periods=1).sum() / vol.rolling(vwap_medium, min_periods=1).sum()\n",
    "\n",
    "    # ÂÖàÈ†≠„ÅÆÂÆâÂÆöÂåñ: Á™ì„ÅåÂüã„Åæ„Å£„Å¶„Å™„ÅÑÊúüÈñì„ÅØ NaN „Å´„Åó„Åü„ÅÑ„Å™„Çâ‰∏ãË®ò„ÇíÊúâÂäπÂåñ\n",
    "    vwap_s = vwap_s.where(vol.rolling(vwap_short).count() >= vwap_short, np.nan)\n",
    "    vwap_m = vwap_m.where(vol.rolling(vwap_medium).count() >= vwap_medium, np.nan)\n",
    "\n",
    "    df[\"VWAP_short\"]  = vwap_s\n",
    "    df[\"VWAP_medium\"] = vwap_m\n",
    "\n",
    "    # --- Regeme: 1/0/NaN (ÂêåÂÄ§„ÅØNaN„Å´„Åó„Å¶„Çπ„Ç§„ÉÉ„ÉÅÊäëÂà∂) ---\n",
    "    reg = np.where(\n",
    "        (df[\"VWAP_short\"] > df[\"VWAP_medium\"]),\n",
    "        1,\n",
    "        np.where(\n",
    "            (df[\"VWAP_short\"] < df[\"VWAP_medium\"]),\n",
    "            0,\n",
    "            np.nan\n",
    "        )\n",
    "    )\n",
    "    df[\"Regeme\"] = reg.astype(float)  # NaN„ÇíÂê´„ÇÄ„Åü„ÇÅ float Âûã\n",
    "\n",
    "    # --- Position „ÅÆÊßãÁØâÔºàÁä∂ÊÖãÊ©üÊ¢∞Ôºâ ---\n",
    "    n = len(df)\n",
    "    pos = np.full(n, np.nan, dtype=float)\n",
    "\n",
    "    have_started = False\n",
    "    current = np.nan\n",
    "\n",
    "    pred = df[\"Pred\"].to_numpy(dtype=float)  # 0/1 ÊÉ≥ÂÆöÔºàNaN„ÇÇË®±ÂÆπÔºâ\n",
    "    rege = df[\"Regeme\"].to_numpy(dtype=float)\n",
    "\n",
    "    for i in range(n):\n",
    "        p = pred[i]\n",
    "        r = rege[i]\n",
    "\n",
    "        # „Å©„Å°„Çâ„Åã„Åå NaN ‚Üí „Çπ„Ç§„ÉÉ„ÉÅ„Åó„Å™„ÅÑ\n",
    "        if np.isnan(p) or np.isnan(r):\n",
    "            if have_started:\n",
    "                pos[i] = current\n",
    "            else:\n",
    "                pos[i] = np.nan\n",
    "            continue\n",
    "\n",
    "        # „Åæ„Å†ÈñãÂßã„Åó„Å¶„ÅÑ„Å™„ÅÑ ‚Üí ‰∏ÄËá¥„Åó„Åü„ÇâÈñãÂßã\n",
    "        if not have_started:\n",
    "            if p == r:\n",
    "                current = p\n",
    "                have_started = True\n",
    "                pos[i] = current\n",
    "            else:\n",
    "                pos[i] = np.nan\n",
    "            continue\n",
    "\n",
    "        # Êó¢„Å´ÈñãÂßãÊ∏à„Åø ‚Üí ‰∏ÄËá¥„Åó„Å¶„ÄÅ„Åã„Å§ÁèæÂú®„Å®ÈÅï„Åà„Å∞ÂàáÊõø\n",
    "        if p == r and current != p:\n",
    "            current = p\n",
    "\n",
    "        pos[i] = current\n",
    "\n",
    "    df[\"Position\"] = pos  # 0/1/NaN\n",
    "\n",
    "    # --- Trade: Position „ÅåÂàá„ÇäÊõø„Çè„Çã„Éù„Ç§„É≥„Éà„Å†„Åë 0/1„ÄÅ„Åù„Çå‰ª•Â§ñ NaN ---\n",
    "    # ÂÖàÈ†≠ÈñãÂßã„ÇÇ„ÄåÂàáÊõø„Äç„Å®„Åø„Å™„Åô\n",
    "    position_series = pd.Series(df[\"Position\"])\n",
    "    changed = position_series.ne(position_series.shift(1))  # Â§âÊõ¥„Éï„É©„Ç∞ÔºàNaNÊØîËºÉ„ÇÇTrue„Å´„Å™„ÇãÔºâ\n",
    "    # „Åü„Å†„Åó„Äå‰∏°ÊñπNaN„Äç„ÅÆ„Å®„Åç„ÅØÂ§âÊõ¥„Åß„ÅØ„Å™„ÅÑ„ÅÆ„ÅßË£úÊ≠£\n",
    "    both_nan = position_series.isna() & position_series.shift(1).isna()\n",
    "    changed = changed & ~both_nan\n",
    "\n",
    "    trade = position_series.where(changed, np.nan)\n",
    "    df[\"Trade\"] = trade\n",
    "\n",
    "    # ÂàóÈ†Ü„ÇíÂ∞ë„ÅóÊï¥„Åà„Å¶Ëøî„ÅôÔºà‰ªªÊÑèÔºâ\n",
    "    desired_order = [\n",
    "        \"timestamp\", \"O\",\"H\",\"L\",\"C\",\"V\",\n",
    "        \"Pred\", \"Label\" if \"Label\" in df.columns else None,\n",
    "        \"VWAP_short\",\"VWAP_medium\",\"Regeme\",\"Position\",\"Trade\"\n",
    "    ]\n",
    "    desired_order = [c for c in desired_order if c is not None]\n",
    "    df = df[desired_order]\n",
    "\n",
    "    return df\n",
    "\n",
    "# === ‰Ωø„ÅÑÊñπ‰æã ===\n",
    "# df_rule = build_vwap_regime_and_trades(df_pred, vwap_short=20, vwap_medium=60, price_col=\"C\", use_typical_price=False)\n",
    "# display(df_rule.head(15))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
